{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**well it is failed experiment for the time bieng**"
      ],
      "metadata": {
        "id": "RGAVUE49_DZ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsd-TL2kmPd3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention, Concatenate, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Load dataset (JSONL)\n",
        "# -----------------------------\n",
        "data_path = \"/content/dataset_t5.jsonl\"  # <- your preprocessed file\n",
        "input_texts, target_texts = [], []\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        ex = json.loads(line)\n",
        "        # Input: \"fsm description: ...\"\n",
        "        input_texts.append(ex[\"input_text\"])\n",
        "        # Target: add start/end markers for decoder supervision\n",
        "        target_texts.append(\"<start> \" + ex[\"target_text\"] + \" <end>\")\n",
        "\n",
        "print(f\"Samples: {len(input_texts)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Tokenization\n",
        "#    (shared tokenizer for simplicity)\n",
        "# -----------------------------\n",
        "tokenizer = Tokenizer(filters=\"\", lower=True, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(input_texts + target_texts)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "# Convert to sequences\n",
        "enc_seqs = tokenizer.texts_to_sequences(input_texts)\n",
        "dec_seqs = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "encoder_input_data = pad_sequences(enc_seqs, padding=\"post\")\n",
        "decoder_input_data = pad_sequences(dec_seqs, padding=\"post\")\n",
        "\n",
        "# Create decoder targets by shifting left by 1\n",
        "decoder_target_data = np.zeros_like(decoder_input_data)\n",
        "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]\n",
        "decoder_target_data[:, -1] = 0\n",
        "\n",
        "max_encoder_len = encoder_input_data.shape[1]\n",
        "max_decoder_len = decoder_input_data.shape[1]\n",
        "print(\"Max input len:\", max_encoder_len, \" Max output len:\", max_decoder_len)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Build ATTENTION Seq2Seq\n",
        "# -----------------------------\n",
        "latent_dim = 256\n",
        "emb_dim = 128\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,), name=\"encoder_inputs\")\n",
        "enc_emb = Embedding(vocab_size, emb_dim, name=\"enc_emb\")(encoder_inputs)\n",
        "# return sequences + states\n",
        "encoder_lstm = LSTM(\n",
        "    latent_dim, return_sequences=True, return_state=True,\n",
        "    dropout=0.2, recurrent_dropout=0.0, name=\"encoder_lstm\"\n",
        ")\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,), name=\"decoder_inputs\")\n",
        "dec_emb = Embedding(vocab_size, emb_dim, name=\"dec_emb\")(decoder_inputs)\n",
        "decoder_lstm = LSTM(\n",
        "    latent_dim, return_sequences=True, return_state=True,\n",
        "    dropout=0.2, recurrent_dropout=0.0, name=\"decoder_lstm\"\n",
        ")\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "# Attention (Luong-style via Keras Attention)\n",
        "attn_layer = Attention(name=\"attention\")\n",
        "context = attn_layer([decoder_outputs, encoder_outputs])  # (batch, dec_len, latent)\n",
        "concat = Concatenate(axis=-1, name=\"concat_ctx\")([decoder_outputs, context])\n",
        "concat = Dropout(0.2)(concat)\n",
        "\n",
        "hidden = Dense(256, activation=\"tanh\")(concat)\n",
        "hidden = Dense(128, activation=\"tanh\")(hidden)\n",
        "decoder_dense = Dense(vocab_size, activation=\"softmax\", name=\"output_dense\")\n",
        "final_outputs = decoder_dense(hidden)\n",
        "\n",
        "# Training model\n",
        "model = Model([encoder_inputs, decoder_inputs], final_outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "model.summary()\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Train (teacher forcing)\n",
        "# -----------------------------\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    np.expand_dims(decoder_target_data, -1),\n",
        "    batch_size=16,\n",
        "    epochs=40,           # bump a bit for stability\n",
        "    validation_split=0.1,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Save training artifacts\n",
        "# -----------------------------\n",
        "save_dir = \"/content/attn_lstm_fsm\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model_path = os.path.join(save_dir, \"attn_lstm_fsm_model.h5\")\n",
        "tok_path = os.path.join(save_dir, \"lstm_tokenizer.pkl\")\n",
        "model.save(model_path)\n",
        "with open(tok_path, \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(\"âœ… Saved:\", model_path, \"and\", tok_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ye predictions ka lia ha and abhi iss say sahi ni aa rahi\n",
        "\n",
        "\n",
        "# Load model & tokenizer if separate\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "\n",
        "model = load_model(\"/content/lstm_fsm_model.h5\")\n",
        "with open(\"/content/lstm_tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "def decode_sequence(input_text, max_output_len=100):\n",
        "    # Encode input\n",
        "    seq = tokenizer.texts_to_sequences([input_text])\n",
        "    seq = pad_sequences(seq, maxlen=model.input[0].shape[1], padding='post')\n",
        "    # Initialize states as zeros (simple decoding)\n",
        "    states = None\n",
        "    target_seq = np.array([[tokenizer.word_index['<start>']]])\n",
        "    output_sentence = []\n",
        "    for _ in range(max_output_len):\n",
        "        preds = model.predict([seq, target_seq], verbose=0)\n",
        "        pred_id = np.argmax(preds[0, -1, :])\n",
        "        word = reverse_word_index.get(pred_id, \"\")\n",
        "        if word == \"<end>\" or word == \"\":\n",
        "            break\n",
        "        output_sentence.append(word)\n",
        "        target_seq = np.array([[pred_id]])\n",
        "    return \" \".join(output_sentence)\n",
        "\n",
        "# ðŸ” Test\n",
        "example_input = \"fsm description: odd number of 1's\"\n",
        "print(\"Generated:\", decode_sequence(example_input))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hiYgDebgwOPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build encoder inference model (reuses training tensors)\n",
        "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
        "\n",
        "# ----- Robustly fetch trained layers we need -----\n",
        "# Decoder LSTM: we already have the layer object as `decoder_lstm` from training.\n",
        "\n",
        "# Decoder Embedding layer: find it from the model (2nd Embedding typically)\n",
        "emb_layers = [l for l in model.layers if isinstance(l, tf.keras.layers.Embedding)]\n",
        "if len(emb_layers) >= 2:\n",
        "    dec_emb_layer = emb_layers[1]\n",
        "else:\n",
        "    dec_emb_layer = emb_layers[-1]  # fallback\n",
        "\n",
        "# Attention layer: grab the single Attention layer used during training\n",
        "attn_layers = [l for l in model.layers if isinstance(l, tf.keras.layers.Attention)]\n",
        "if not attn_layers:\n",
        "    raise RuntimeError(\"Could not find Attention layer in the trained model.\")\n",
        "attn_layer_trained = attn_layers[0]\n",
        "\n",
        "# Dense stack at the end: last three Dense layers in training were:\n",
        "#   dense_hidden1 -> dense_hidden2 -> decoder_dense(softmax)\n",
        "dense_layers = [l for l in model.layers if isinstance(l, tf.keras.layers.Dense)]\n",
        "if len(dense_layers) < 3:\n",
        "    raise RuntimeError(\"Expected at least 3 Dense layers (2 hidden + output) in the trained model.\")\n",
        "dense1, dense2, dense_out = dense_layers[-3], dense_layers[-2], dense_layers[-1]\n",
        "\n",
        "# ----- Decoder inference graph (one token at a time) -----\n",
        "# Inputs\n",
        "inf_decoder_input = Input(shape=(1,), name=\"inf_decoder_input\")  # previous token id\n",
        "inf_state_h = Input(shape=(latent_dim,), name=\"inf_state_h\")\n",
        "inf_state_c = Input(shape=(latent_dim,), name=\"inf_state_c\")\n",
        "inf_encoder_outputs = Input(shape=(max_encoder_len, latent_dim), name=\"inf_encoder_outputs\")\n",
        "\n",
        "# Embed current token with the *trained* decoder embedding\n",
        "inf_dec_emb = dec_emb_layer(inf_decoder_input)\n",
        "\n",
        "# Run trained decoder LSTM one step with incoming states\n",
        "inf_dec_outputs, out_h, out_c = decoder_lstm(inf_dec_emb, initial_state=[inf_state_h, inf_state_c])\n",
        "\n",
        "# Apply the *trained* attention over encoder outputs\n",
        "inf_context = attn_layer_trained([inf_dec_outputs, inf_encoder_outputs])\n",
        "\n",
        "# Concatenate decoder step with its context, then pass through trained dense stack\n",
        "inf_concat = Concatenate(axis=-1, name=\"inf_concat\")([inf_dec_outputs, inf_context])\n",
        "x = dense1(inf_concat)\n",
        "x = dense2(x)\n",
        "inf_token_probs = dense_out(x)  # softmax over vocab\n",
        "\n",
        "decoder_model = Model(\n",
        "    [inf_decoder_input, inf_state_h, inf_state_c, inf_encoder_outputs],\n",
        "    [inf_token_probs, out_h, out_c]\n",
        ")\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# -------- helpers --------\n",
        "def _clean_join(tokens):\n",
        "    text = \" \".join([t for t in tokens if t])\n",
        "    return text.replace(\" ,\", \",\").replace(\" ;\", \";\").strip()\n",
        "\n",
        "def _dedup_transitions(text):\n",
        "    \"\"\"\n",
        "    Remove duplicate transitions while preserving order.\n",
        "    Works whether your text has 'transitions:' header or just rules separated by ';'.\n",
        "    \"\"\"\n",
        "    if \"transitions:\" in text:\n",
        "        head, tail = text.split(\"transitions:\", 1)\n",
        "        items = [s.strip() for s in tail.split(\";\") if s.strip()]\n",
        "        seen, out = set(), []\n",
        "        for it in items:\n",
        "            key = re.sub(r\"\\s+\", \"\", it.lower())\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                out.append(it)\n",
        "        return (head + \"transitions: \" + \"; \".join(out)).strip()\n",
        "    else:\n",
        "        items = [s.strip() for s in text.split(\";\") if s.strip()]\n",
        "        seen, out = set(), []\n",
        "        for it in items:\n",
        "            key = re.sub(r\"\\s+\", \"\", it.lower())\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                out.append(it)\n",
        "        return \"; \".join(out)\n",
        "\n",
        "def _no_repeat_ngram_ok(prev_ids, cand_id, n=3):\n",
        "    if len(prev_ids) < n - 1:\n",
        "        return True\n",
        "    ctx = prev_ids[-(n-1):] + [cand_id]\n",
        "    for i in range(len(prev_ids) - (n-1)):\n",
        "        if prev_ids[i:i+n-1] + [cand_id] == ctx:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def _pick_with_constraints(probs, prev_ids, recent_window=30, repetition_penalty=1.2,\n",
        "                           no_repeat_ngram=3, ban_ids=None, top_k=50):\n",
        "    p = probs.copy().astype(np.float64) + 1e-12\n",
        "    # repetition penalty on recent tokens\n",
        "    if repetition_penalty > 1.0 and len(prev_ids) > 0:\n",
        "        recent = set(prev_ids[-recent_window:])\n",
        "        for rid in recent:\n",
        "            if 0 <= rid < p.shape[-1]:\n",
        "                p[rid] = p[rid] / repetition_penalty\n",
        "    # ban ids (pad/start/unk)\n",
        "    if ban_ids:\n",
        "        for bid in ban_ids:\n",
        "            if 0 <= bid < p.shape[-1]:\n",
        "                p[bid] = 0.0\n",
        "    # shortlist top-k\n",
        "    top_k = min(top_k, p.shape[-1])\n",
        "    top_ids = np.argpartition(-p, top_k-1)[:top_k]\n",
        "    top_ids = top_ids[np.argsort(-p[top_ids])]\n",
        "    # pick first that satisfies no-repeat-ngram\n",
        "    for tid in top_ids:\n",
        "        if no_repeat_ngram and not _no_repeat_ngram_ok(prev_ids, int(tid), n=no_repeat_ngram):\n",
        "            continue\n",
        "        return int(tid)\n",
        "    return int(top_ids[0]) if len(top_ids) else int(np.argmax(p))\n",
        "\n",
        "# -------- Constrained decoding + post-dedup --------\n",
        "def generate_fsm(prompt: str, max_len: int = 140, repeat_stop: int = 6,\n",
        "                 no_repeat_ngram: int = 3, top_k: int = 50, repetition_penalty: float = 1.2) -> str:\n",
        "    \"\"\"\n",
        "    Uses no-repeat n-gram, repetition penalty, and top-k to reduce loops.\n",
        "    Then removes duplicate transitions in post-processing.\n",
        "    \"\"\"\n",
        "    # Encode input once\n",
        "    seq = tokenizer.texts_to_sequences([prompt])\n",
        "    seq = pad_sequences(seq, maxlen=max_encoder_len, padding=\"post\")\n",
        "    enc_outs, h, c = encoder_model.predict(seq, verbose=0)\n",
        "\n",
        "    # Start with <start>\n",
        "    target_id = start_id\n",
        "    out_ids, out_tokens = [], []\n",
        "    same_count, last_token = 0, None\n",
        "\n",
        "    ban_ids = [0]  # pad\n",
        "    if start_id is not None: ban_ids.append(start_id)\n",
        "    if unk_id   is not None: ban_ids.append(unk_id)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        token_probs, h, c = decoder_model.predict(\n",
        "            [np.array([[target_id]]), h, c, enc_outs], verbose=0\n",
        "        )\n",
        "        probs = token_probs[0, -1, :]\n",
        "\n",
        "        next_id = _pick_with_constraints(\n",
        "            probs=probs,\n",
        "            prev_ids=out_ids,\n",
        "            recent_window=30,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            no_repeat_ngram=no_repeat_ngram,\n",
        "            ban_ids=ban_ids,\n",
        "            top_k=top_k\n",
        "        )\n",
        "\n",
        "        # stopping\n",
        "        if end_id is not None and next_id == end_id:\n",
        "            break\n",
        "        if next_id == last_token:\n",
        "            same_count += 1\n",
        "            if same_count >= repeat_stop:\n",
        "                break\n",
        "        else:\n",
        "            same_count = 0\n",
        "        last_token = next_id\n",
        "\n",
        "        out_ids.append(next_id)\n",
        "        tok = idx2word.get(next_id, \"\")\n",
        "        if tok:\n",
        "            out_tokens.append(tok)\n",
        "\n",
        "        target_id = next_id\n",
        "\n",
        "    raw_text = _clean_join(out_tokens)\n",
        "    final_text = _dedup_transitions(raw_text)\n",
        "    return final_text\n",
        "\n",
        "# Quick check\n",
        "print(generate_fsm(\"fsm description: odd number of 1's\"))\n"
      ],
      "metadata": {
        "id": "jfsD1CxEw_cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zPqD7FT6Nqc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}