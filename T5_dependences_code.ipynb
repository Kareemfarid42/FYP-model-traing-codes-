{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dependences model traing code :\n",
        "\n",
        "\n",
        "Traing the model to only predict the first column:\n",
        "Stage 1"
      ],
      "metadata": {
        "id": "uKsm0Bj5Iui1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install datasets transformers rouge-score nltk\n",
        "#!pip install evaluate\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "file_path = \"/content/dataset_FYP.xlsx\"\n",
        "\n",
        "# Load Excel file\n",
        "df = pd.read_excel(file_path, engine='openpyxl')\n",
        "df=df[[\"Question statment\",\"Characters\",\"States\",\"Start State\",\"Transitions\",\"Final States\"]]\n",
        "\n",
        "df.head()\n",
        "\n",
        "\n",
        "df[\"Question statment\"]=df[\"Question statment\"].str.replace('\"', ' ')\n",
        "df[\"Characters\"]=df[\"Characters\"].str.replace('\"', '').str.strip()\n",
        "df[\"States\"]=df[\"States\"].str.replace('\"', '').str.strip()\n",
        "df[\"Start State\"]=df[\"Start State\"].str.replace('\"', '').str.strip()\n",
        "df[\"Transitions\"]=df[\"Transitions\"].str.replace('\"', '').str.strip()\n",
        "df[\"Final States\"]=df[\"Final States\"].str.replace('\"', '').str.strip()\n",
        "\n",
        "\n",
        "\n",
        "df[\"States\"] = df[\"States\"].str.replace(\" \", \"\").str.replace(\"{\", \"\").str.replace(\"}\", \"\")\n",
        "df[\"States\"] = \"{\" + df[\"States\"] + \"}\"\n",
        "stage_1=df[[\"Question statment\",\"Characters\"]]\n",
        "\n",
        "stage_1 = df[[\"Question statment\", \"Characters\"]].dropna()\n",
        "stage_1 = stage_1[(stage_1[\"Question statment\"].str.strip() != \"\") & (stage_1[\"Characters\"].str.strip() != \"\")]\n",
        "\n",
        "stage_1 = stage_1.rename(columns={\n",
        "    \"Question statment\": \"input_text\",\n",
        "    \"Characters\": \"target_text\"\n",
        "})\n",
        "\n",
        "\n",
        "dataset1 = Dataset.from_pandas(stage_1)\n",
        "\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess(example):\n",
        "    model_input = tokenizer(\"Predict characters: \" + example[\"input_text\"],\n",
        "                            truncation=True, padding=\"max_length\", max_length=64)\n",
        "    target = tokenizer(example[\"target_text\"],\n",
        "                       truncation=True, padding=\"max_length\", max_length=16)\n",
        "    model_input[\"labels\"] = target[\"input_ids\"]\n",
        "    return model_input\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_dataset1 = dataset1.map(preprocess)\n",
        "\n",
        "\n",
        "split = tokenized_dataset1.train_test_split(test_size=0.1,shuffle=True,seed=42)\n",
        "train_dataset = split[\"train\"]\n",
        "eval_dataset = split[\"test\"]\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    predictions, labels = eval_preds\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds = [p.strip() for p in decoded_preds]\n",
        "    decoded_labels = [l.strip() for l in decoded_labels]\n",
        "    return {\"accuracy\": accuracy_score(decoded_labels, decoded_preds)}\n",
        "\n",
        "\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./t5_stage1_model\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=30,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",            # <-- Log every epoch\n",
        "    logging_dir=\"./logs\",\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"tensorboard\"              # <-- Enables TensorBoard logging\n",
        ")\n",
        "\n",
        "# Step 6: Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
        "    compute_metrics=compute_metrics,     # <-- Include accuracy metric\n",
        ")\n",
        "\n",
        "# Step 7: Train\n",
        "trainer.train()\n",
        "\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "CA3ZfmibHlLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOJKV0HzUMAT"
      },
      "outputs": [],
      "source": [
        "#!unzip -q t5_stage1_model.zip -d t5_stage1_model/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the stage 1 predictions"
      ],
      "metadata": {
        "id": "7RE9xuxQJkCS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzSv1pYVtBBy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load trained model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/T5_stage1_model\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Example test input (similar format to your \"Question statement\")\n",
        "input_question = \"Strings of length atleast 3\"\n",
        "\n",
        "# Prepare input for model\n",
        "input_text = \"Predict characters: \" + input_question\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate prediction\n",
        "output_ids = model.generate(input_ids, max_length=30, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode output\n",
        "predicted_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Predicted Characters:\", \"{\"+predicted_text+\"}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIKx6uzZLr60"
      },
      "source": [
        "### **Stage 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTOb3GBQUKU8"
      },
      "outputs": [],
      "source": [
        "# ===== Step 1: Load & Prepare Data =====\n",
        "df = df[[\"Question statment\", \"States\"]].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn_jA2CCegH1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "stage_2 = df.dropna(subset=[\"Question statment\", \"States\"])\n",
        "stage_2 = stage_2[(stage_2[\"Question statment\"].str.strip() != \"\") & (stage_2[\"States\"].str.strip() != \"\")]\n",
        "\n",
        "\n",
        "\n",
        "# ===== Step 2: Create Dataset =====\n",
        "stage_2 = pd.DataFrame({\n",
        "    \"input_text\": stage_2[\"Question statment\"],\n",
        "    \"target_text\": stage_2[\"States\"]\n",
        "})\n",
        "\n",
        "\n",
        "dataset = Dataset.from_pandas(stage_2)\n",
        "\n",
        "# ===== Step 3: Tokenizer & Preprocessing =====\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess(example):\n",
        "    model_input = tokenizer(\n",
        "        \"Given the DFA description, list all states in {q0,q1,...} format: \" + example[\"input_text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target = tokenizer(\n",
        "            example[\"target_text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=64\n",
        "        )\n",
        "    model_input[\"labels\"] = target[\"input_ids\"]\n",
        "    return model_input\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess)\n",
        "\n",
        "# ===== Step 4: Train-Test Split =====\n",
        "split = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
        "train_dataset = split[\"train\"]\n",
        "eval_dataset = split[\"test\"]\n",
        "\n",
        "# ===== Step 5: Metrics =====\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    predictions, labels = eval_preds\n",
        "\n",
        "    # Convert model predictions to text\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels before decoding\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Strip spaces\n",
        "    decoded_preds = [p.strip() for p in decoded_preds]\n",
        "    decoded_labels = [l.strip() for l in decoded_labels]\n",
        "\n",
        "    # Compute accuracy\n",
        "    return {\"accuracy\": accuracy_score(decoded_labels, decoded_preds)}\n",
        "\n",
        "\n",
        "# ===== Step 6: Training Setup =====\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"t5_stage2_model\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=20,  # More epochs since dataset is small\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=1000,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ===== Step 7: Train =====\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Step 7: Save =====\n",
        "model.save_pretrained(\"t5_stage2_model\")\n",
        "tokenizer.save_pretrained(\"t5_stage2_model\")"
      ],
      "metadata": {
        "id": "UZmgKP1E3DN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV8J0fZ0c-QS"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trying the fold Method for stage 2**"
      ],
      "metadata": {
        "id": "Ga9NVQegKuwL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb7fsgNuBUPq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ===== Load & Clean Data =====\n",
        "df = pd.read_excel(\"/content/dataset_FYP.xlsx\")  # Replace with your dataset path\n",
        "df = df.dropna(subset=[\"Question statment\", \"States\"])\n",
        "df = df[(df[\"Question statment\"].str.strip() != \"\") & (df[\"States\"].str.strip() != \"\")]\n",
        "\n",
        "# Create Hugging Face Dataset\n",
        "hf_dataset = Dataset.from_pandas(pd.DataFrame({\n",
        "    \"input_text\": df[\"Question statment\"],\n",
        "    \"target_text\": df[\"States\"]\n",
        "}))\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess(example):\n",
        "    model_input = tokenizer(\n",
        "        \"Given the DFA description, list all states in {q0,q1,...} format: \" + example[\"input_text\"],\n",
        "        truncation=True, padding=\"max_length\", max_length=128\n",
        "    )\n",
        "    target = tokenizer(\n",
        "        example[\"target_text\"],\n",
        "        truncation=True, padding=\"max_length\", max_length=64\n",
        "    )\n",
        "    model_input[\"labels\"] = target[\"input_ids\"]\n",
        "    return model_input\n",
        "\n",
        "# Pre-tokenize dataset\n",
        "tokenized_dataset = hf_dataset.map(preprocess)\n",
        "\n",
        "# Accuracy function\n",
        "def compute_metrics(eval_preds):\n",
        "    predictions, labels = eval_preds\n",
        "    predictions = predictions.argmax(-1) if predictions.ndim == 3 else predictions\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    return {\"accuracy\": accuracy_score([l.strip() for l in decoded_labels],\n",
        "                                       [p.strip() for p in decoded_preds])}\n",
        "\n",
        "# ===== K-Fold Cross Validation =====\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(tokenized_dataset)):\n",
        "    print(f\"\\n===== Fold {fold+1} =====\")\n",
        "\n",
        "    train_dataset = tokenized_dataset.select(train_idx)\n",
        "    eval_dataset = tokenized_dataset.select(val_idx)\n",
        "\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=f\"t5_stage2_fold{fold+1}\",\n",
        "        learning_rate=5e-5,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        num_train_epochs=10,\n",
        "        weight_decay=0.01,\n",
        "        predict_with_generate=True,\n",
        "        logging_dir=f\"./logs_fold{fold+1}\",\n",
        "        save_total_limit=1\n",
        "    )\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    print(f\"Fold {fold+1} Accuracy:\", metrics[\"eval_accuracy\"])\n",
        "    fold_results.append(metrics[\"eval_accuracy\"])\n",
        "\n",
        "# ===== Final Average Accuracy =====\n",
        "print(\"\\nAverage Accuracy:\", sum(fold_results)/len(fold_results))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}