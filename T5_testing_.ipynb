{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Using T5 without dependences lets see how well this works**"
      ],
      "metadata": {
        "id": "oBTkTb2wO-kK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrzYWd3j2V-4"
      },
      "outputs": [],
      "source": [
        "# file: preprocess_t5_dataset_cleaned.py\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# 1. Load Excel dataset\n",
        "df = pd.read_excel(\"/content/dataset_FYP.xlsx\")\n",
        "\n",
        "# 2. Clean generic text\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"[\\\" '\\[\\]\\(\\)\\{\\}]\", \"\", text)  # remove brackets, quotes\n",
        "    text = re.sub(r\"\\s+\", \" \", text)               # remove extra spaces/newlines\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)      # remove non-ASCII chars\n",
        "    return text\n",
        "\n",
        "# 3. Normalize FSM transitions from \"{(q0,0,q0),...}\" → \"q0,0->q0; q0,1->q1; ...\"\n",
        "def normalize_transitions(transitions: str) -> str:\n",
        "    if not isinstance(transitions, str):\n",
        "        return \"\"\n",
        "\n",
        "    transitions = transitions.strip().strip(\"\\\"'{}\")\n",
        "\n",
        "\n",
        "    transitions = transitions.replace(\"(\", \"\").replace(\")\", \"\")\n",
        "\n",
        "    parts = [p.strip() for p in transitions.split(\",\") if p.strip()]\n",
        "\n",
        "\n",
        "    grouped = []\n",
        "    for i in range(0, len(parts), 3):\n",
        "        if i + 2 < len(parts):\n",
        "            #print(parts[i+2])\n",
        "            src, symbol, dest = parts[i], parts[i+1], parts[i+2]\n",
        "            grouped.append(f\"{src},{symbol}->{dest}\")\n",
        "\n",
        "    #print(grouped)\n",
        "    clean_transitions = \"; \".join(grouped)\n",
        "\n",
        "    #print(clean_transitions)\n",
        "\n",
        "    return clean_transitions\n",
        "\n",
        "\n",
        "# 4. Apply cleaning and normalization\n",
        "df[\"Question statment\"] = df[\"Question statment\"].apply(clean_text)\n",
        "df[\"Start State\"] = df[\"Start State\"].apply(clean_text)\n",
        "df[\"Final States\"] = df[\"Final States\"].apply(clean_text)\n",
        "df[\"Transitions\"] = df[\"Transitions\"].apply(lambda x: normalize_transitions(clean_text(x)))\n",
        "\n",
        "#print(df['Transitions'])\n",
        "\n",
        "df = df.dropna(subset=[\"Question statment\", \"Start State\", \"Final States\", \"Transitions\"])\n",
        "df = df[\n",
        "    (df[\"Question statment\"].str.strip() != \"\") &\n",
        "    (df[\"Start State\"].str.strip() != \"\") &\n",
        "    (df[\"Final States\"].str.strip() != \"\") &\n",
        "    (df[\"Transitions\"].str.strip() != \"\")\n",
        "]\n",
        "\n",
        "#print(df['Transitions'])\n",
        "# 5. Build T5 input/output pairs\n",
        "pairs = []\n",
        "for _, row in df.iterrows():\n",
        "    question = row[\"Question statment\"]\n",
        "    start = row[\"Start State\"]\n",
        "    final = row[\"Final States\"]\n",
        "    transitions = row[\"Transitions\"]\n",
        "\n",
        "    input_text = f\"fsm description: {question}\"\n",
        "    output_text = f\"start: {start}; final: {final}; transitions: {transitions}\"\n",
        "    #print(output_text)\n",
        "    pairs.append({\n",
        "        \"input_text\": input_text,\n",
        "        \"target_text\": output_text\n",
        "    })\n",
        "# 6. Save as JSONL (HuggingFace compatible)\n",
        "output_path = \"/content/dataset_t5.jsonl\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for p in pairs:\n",
        "        #print(p)\n",
        "        json.dump(p, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q transformers datasets accelerate peft evaluate rouge_score\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# 2) Config\n",
        "dataset_path = \"/content/dataset_t5.jsonl\"\n",
        "model_name = \"t5-small\"\n",
        "output_dir = \"./t5-finetuned-fsm\"\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "# 3) Load dataset and split\n",
        "ds = load_dataset(\"json\", data_files=dataset_path)\n",
        "ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "print(f\"Train size: {len(ds['train'])}, Eval size: {len(ds['test'])}\")\n",
        "\n",
        "# 4) Tokenizer & model - FIXED\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# 5) Preprocessing with safety checks - FIXED\n",
        "def preprocess_batch(batch):\n",
        "    # Tokenize inputs\n",
        "    inputs = tokenizer(\n",
        "        batch[\"input_text\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Tokenize targets\n",
        "    labels = tokenizer(\n",
        "        batch[\"target_text\"],\n",
        "        max_length=max_target_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Convert labels and ensure valid token IDs - FIXED\n",
        "    labels_input_ids = []\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    for seq in labels[\"input_ids\"]:\n",
        "        new_seq = []\n",
        "        for token_id in seq:\n",
        "            if token_id == tokenizer.pad_token_id:\n",
        "                new_seq.append(-100)\n",
        "            elif token_id >= vocab_size:\n",
        "                # Replace out-of-vocabulary tokens with unk_token_id\n",
        "                new_seq.append(tokenizer.unk_token_id)\n",
        "            else:\n",
        "                new_seq.append(token_id)\n",
        "        labels_input_ids.append(new_seq)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"labels\": labels_input_ids,\n",
        "    }\n",
        "\n",
        "tokenized = ds.map(preprocess_batch, batched=True, remove_columns=ds[\"train\"].column_names)\n",
        "\n",
        "# 6) VERIFY TOKEN IDs BEFORE TRAINING - ADD THIS\n",
        "print(\"Verifying token IDs...\")\n",
        "vocab_size = len(tokenizer)\n",
        "max_input_id = max(max(seq) for seq in tokenized[\"train\"][\"input_ids\"])\n",
        "max_label_id = max(max(seq) for seq in tokenized[\"train\"][\"labels\"] if any(tok != -100 for tok in seq))\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Max input token ID: {max_input_id}\")\n",
        "print(f\"Max label token ID: {max_label_id}\")\n",
        "\n",
        "if max_input_id >= vocab_size or max_label_id >= vocab_size:\n",
        "    print(\"⚠️  WARNING: Some token IDs exceed vocabulary size!\")\n",
        "    # Apply emergency fix\n",
        "    def fix_token_ids(example):\n",
        "        example[\"input_ids\"] = [min(token_id, vocab_size - 1) for token_id in example[\"input_ids\"]]\n",
        "        example[\"labels\"] = [min(token_id, vocab_size - 1) if token_id != -100 else -100 for token_id in example[\"labels\"]]\n",
        "        return example\n",
        "\n",
        "    tokenized = tokenized.map(fix_token_ids)\n",
        "    print(\"✅ Applied emergency token ID fix\")\n",
        "else:\n",
        "    print(\"✅ All token IDs are within valid range\")\n",
        "\n",
        "# 7) Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n",
        "\n",
        "# 8) Enable LoRA + gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# 9) Metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def normalize_text(s):\n",
        "    return \" \".join(s.lower().strip().split()) if isinstance(s, str) else \"\"\n",
        "\n",
        "def postprocess_texts(preds, labels):\n",
        "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    preds = [normalize_text(p) for p in preds]\n",
        "    labels = [normalize_text(l) for l in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    if isinstance(preds, tuple): preds = preds[0]\n",
        "    if preds.ndim == 3: preds = np.argmax(preds, axis=-1)\n",
        "    preds, labels = postprocess_texts(preds, labels)\n",
        "    rouge_result = rouge.compute(predictions=preds, references=labels, use_stemmer=True)\n",
        "    exact_match = np.mean([p == l for p, l in zip(preds, labels)])\n",
        "    return {\n",
        "        \"rouge1\": float(rouge_result[\"rouge1\"]),\n",
        "        \"rougeL\": float(rouge_result[\"rougeL\"]),\n",
        "        \"exact_match\": float(exact_match),\n",
        "    }\n",
        "\n",
        "# 10) Training arguments - FIXED TYPOS\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=max_target_length,\n",
        "    eval_strategy=\"no\",  # FIXED: was \"eval_strategy\"\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    num_train_epochs=6,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    weight_decay=0.01,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# 11) Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "# 13) Save model and adapters\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "model.save_pretrained(output_dir + \"/peft_adapters\")\n",
        "\n",
        "print(\"✅ Training finished and saved to\", output_dir)\n",
        "\n",
        "# 14) Quick inference test\n",
        "def generate_fsm(prompt: str, max_new_tokens: int = 128):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_input_length).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_length=max_target_length, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "example_prompt = \"fsm description: odd number of 1's\"\n",
        "print(\"EXAMPLE INFERENCE:\", generate_fsm(example_prompt))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5A9SICMA2oq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate() #piece id is out of range. ye error aa raha ha evaluation ma iss ki samjh ni aa rahi mujhay kesy sahi karu\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "g3O5QFWZp5rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CICGMpsYreCZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}