{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Using GPT-2 for model Traing**"
      ],
      "metadata": {
        "id": "LbAhoOvDOthZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch_xla[tpu] -f https://storage.googleapis.com/tpu-pytorch/wheels/colab.html"
      ],
      "metadata": {
        "id": "8uO1uunsj5Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE-g6aIIWgQO"
      },
      "source": [
        "setting the dataset format to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWRch6PeQVPR"
      },
      "outputs": [],
      "source": [
        "# file: preprocess_dataset.py\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "# Load dataset\n",
        "file_path = \"/content/dataset_FYP.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
        "\n",
        "# Clean helper\n",
        "def clean_text(text: str) -> str:\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    return re.sub(r'[\"{}]', '', str(text)).strip()\n",
        "\n",
        "# Build input-output pairs\n",
        "pairs = []\n",
        "for _, row in tqdm(df.iterrows()):\n",
        "    question = clean_text(row[\"Question statment\"])\n",
        "    start = clean_text(row[\"Start State\"])\n",
        "    final = clean_text(row[\"Final States\"])\n",
        "    transitions = clean_text(row[\"Transitions\"])\n",
        "\n",
        "    # Normalize transitions into list format\n",
        "    transitions = transitions.replace(\"),\", \")|\")  # temporary separator\n",
        "    transitions = transitions.replace(\"(\", \"[\").replace(\")\", \"]\")\n",
        "    transitions = transitions.replace(\"|\", \", \")\n",
        "\n",
        "    input_text = f\"INPUT: {question}\"\n",
        "    output_text = f\"OUTPUT: START={start}; FINAL={final}; TRANSITIONS={transitions}\"\n",
        "\n",
        "    pairs.append({\"input\": input_text, \"output\": output_text})\n",
        "\n",
        "# Save as JSONL for HuggingFace\n",
        "jsonl_path = \"dataset_gpt2.jsonl\"\n",
        "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for p in pairs:\n",
        "        f.write(json.dumps(p) + \"\\n\")\n",
        "\n",
        "# Also save as plain TXT (optional)\n",
        "txt_path = \"dataset_gpt2.txt\"\n",
        "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for p in pairs:\n",
        "        f.write(p[\"input\"] + \"\\n\" + p[\"output\"] + \"\\n\\n\")\n",
        "\n",
        "print(f\"Saved {len(pairs)} samples to {jsonl_path} and {txt_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmlHWTXaWOSu"
      },
      "outputs": [],
      "source": [
        "# file: finetune_gpt2.py\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "# 1. Load dataset (JSONL format)\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/dataset_gpt2.jsonl\")\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)  # 80/20 split\n",
        "\n",
        "# 2. Load tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 has no pad token\n",
        "\n",
        "# 3. Preprocess function\n",
        "def preprocess(example):\n",
        "    text = f\"INPUT: {example['input']} OUTPUT: {example['output']}\"\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "    )\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, batched=False)\n",
        "\n",
        "# 4. Load model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 5. Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "# 6. Accuracy metric (exact match)\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Replace -100 with pad_token_id\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Exact match: all tokens equal\n",
        "    matches = (predictions == labels).all(axis=1).astype(float)\n",
        "    acc = matches.mean()\n",
        "\n",
        "    return {\"exact_match_acc\": acc}\n",
        "\n",
        "# 7. Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned-fsm\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",  # Evaluate each epoch\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "# 8. Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 9. Train\n",
        "trainer.train()\n",
        "\n",
        "# 10. Save model + tokenizer\n",
        "#trainer.save_model(\"./gpt2-finetuned-fsm\")\n",
        "#tokenizer.save_pretrained(\"./gpt2-finetuned-fsm\")\n",
        "\n",
        "print(\"âœ… Training complete. Model + tokenizer saved at ./gpt2-finetuned-fsm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "having memory issues need to consult some one about how to solve this"
      ],
      "metadata": {
        "id": "SoqwJBvpOQlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtPJ-sP0Q_5f"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"/content/dataset_gpt2.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K_X4G_bRqyC"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"].features[\"input\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zutIXyFRkaRR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}